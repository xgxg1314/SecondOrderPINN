{"cells":[{"cell_type":"markdown","metadata":{"id":"gTYjYJsNpLj7"},"source":["# Attribute\n","\n","**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n","\n","**Github Repo** : https://github.com/maziarraissi/PINNs\n","\n","**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n","\n","@article{raissi2017physicsI,\n","  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n","  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n","  journal={arXiv preprint arXiv:1711.10561},\n","  year={2017}\n","}\n","\n","@article{raissi2017physicsII,\n","  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n","  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n","  journal={arXiv preprint arXiv:1711.10566},\n","  year={2017}\n","}"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmx-Lrh7qTAo","executionInfo":{"status":"ok","timestamp":1653307864397,"user_tz":-480,"elapsed":24774,"user":{"displayName":"xiong xiong","userId":"17756750327085610403"}},"outputId":"96f0833f-60d2-44f2-fe93-172549962e0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content\n"]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Colab Notebooks/p22_pyhessianPINN/Inverse_PINN\"\n","import os\n","\n","os.chdir(path)\n","\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69I6dQbKqbZB","executionInfo":{"status":"ok","timestamp":1653307864864,"user_tz":-480,"elapsed":472,"user":{"displayName":"xiong xiong","userId":"17756750327085610403"}},"outputId":"fdce4fff-5080-449b-d891-f24317080633"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/p22_pyhessianPINN/Inverse_PINN\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"_TXaS8DjqblD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch_optimizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZONr5k0UplsT","executionInfo":{"status":"ok","timestamp":1653307868293,"user_tz":-480,"elapsed":3432,"user":{"displayName":"xiong xiong","userId":"17756750327085610403"}},"outputId":"71003e71-ca8f-4a18-9da1-529970fd668d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_optimizer\n","  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 420 kB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.11.0+cu113)\n","Collecting pytorch-ranger>=0.1.1\n","  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (4.2.0)\n","Installing collected packages: pytorch-ranger, torch-optimizer\n","Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"]}]},{"cell_type":"code","source":["\n","# ================ 安装新的优化器库 ============================\n","import torch_optimizer as optim\n","\n"],"metadata":{"id":"VuyY2L-bwiXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","import torch_optimizer as optim\n","\n","# model = ...\n","optimizer = optim.Adahessian(\n","    m.parameters(),\n","    lr= 1.0,\n","    betas= (0.9, 0.999)\n","    eps= 1e-4,\n","    weight_decay=0.0,\n","    hessian_power=1.0,\n",")\n","      loss_fn(m(input), target).backward(create_graph = True) # create_graph=True is necessary for Hessian calculation\n","optimizer.step()\n","\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"MxmgfgqztNe7","executionInfo":{"status":"ok","timestamp":1653307871164,"user_tz":-480,"elapsed":515,"user":{"displayName":"xiong xiong","userId":"17756750327085610403"}},"outputId":"ad281917-eefa-4b95-819e-7892eecad33c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport torch_optimizer as optim\\n\\n# model = ...\\noptimizer = optim.Adahessian(\\n    m.parameters(),\\n    lr= 1.0,\\n    betas= (0.9, 0.999)\\n    eps= 1e-4,\\n    weight_decay=0.0,\\n    hessian_power=1.0,\\n)\\n      loss_fn(m(input), target).backward(create_graph = True) # create_graph=True is necessary for Hessian calculation\\noptimizer.step()\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"NEGmUDVspLkB"},"source":["## Libraries and Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkKp7Kf4pLkC"},"outputs":[],"source":["import sys\n","\n","# sys.path.insert(0, '../Utilities/')\n","\n","import torch\n","from collections import OrderedDict\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy.interpolate import griddata\n","\n","# from plotting import newfig, savefig\n","\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","import matplotlib.gridspec as gridspec\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","np.random.seed(1234)\n","\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"rY5VWIqBplAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDjixYytpLkE"},"outputs":[],"source":["# CUDA support \n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')"]},{"cell_type":"markdown","metadata":{"id":"_J3hZlIqpLkF"},"source":["## Physics-informed Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQTG74aspLkG"},"outputs":[],"source":["# the deep neural network\n","class DNN(torch.nn.Module):\n","    def __init__(self, layers):\n","        super(DNN, self).__init__()\n","        \n","        # parameters\n","        self.depth = len(layers) - 1\n","        \n","        # set up layer order dict\n","        self.activation = torch.nn.Tanh\n","        \n","        layer_list = list()\n","        for i in range(self.depth - 1): \n","            layer_list.append(\n","                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n","            )\n","            layer_list.append(('activation_%d' % i, self.activation()))\n","            \n","        layer_list.append(\n","            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n","        )\n","        layerDict = OrderedDict(layer_list)\n","        \n","        # deploy layers\n","        self.layers = torch.nn.Sequential(layerDict)\n","        \n","    def forward(self, x):\n","        out = self.layers(x)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yk8zKRgGpLkH"},"outputs":[],"source":["# the physics-guided neural network\n","class PhysicsInformedNN():\n","    def __init__(self, X, u, layers, lb, ub):\n","        \n","        # boundary conditions\n","        self.lb = torch.tensor(lb).float().to(device)\n","        self.ub = torch.tensor(ub).float().to(device)\n","        \n","        # data\n","        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n","        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n","        self.u = torch.tensor(u).float().to(device)\n","        \n","        # settings\n","        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n","        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n","        \n","        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n","        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n","        \n","        # deep neural networks\n","        self.dnn = DNN(layers).to(device)\n","        self.dnn.register_parameter('lambda_1', self.lambda_1)\n","        self.dnn.register_parameter('lambda_2', self.lambda_2)\n","        \n","         # optimizers: using the same settings\n","        self.optimizer = torch.optim.LBFGS(\n","            self.dnn.parameters(), \n","            lr=1.0, \n","            max_iter=0,   # 50000\n","            max_eval=0, # 50000 \n","            history_size=50,\n","            tolerance_grad=1e-5, \n","            tolerance_change=1.0 * np.finfo(float).eps,\n","            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n","        )\n","        \n","        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n","        self.iter = 0\n","\n","        # ============== 增加新的优化器：二阶优化器 ==================\n","        self.optimizer_Adahessian = optim.Adahessian(\n","                                self.dnn.parameters(),\n","                                lr= 1.0,\n","                                betas= (0.9, 0.999),\n","                                eps= 1e-4,\n","                                weight_decay=0.0,\n","                                hessian_power=1.0,)\n","        \n","    def net_u(self, x, t):  \n","        u = self.dnn(torch.cat([x, t], dim=1))\n","        return u\n","    \n","    def net_f(self, x, t):\n","        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n","        lambda_1 = self.lambda_1        \n","        lambda_2 = torch.exp(self.lambda_2)\n","        u = self.net_u(x, t)\n","        \n","        u_t = torch.autograd.grad(\n","            u, t, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","        u_x = torch.autograd.grad(\n","            u, x, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","        u_xx = torch.autograd.grad(\n","            u_x, x, \n","            grad_outputs=torch.ones_like(u_x),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","        \n","        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n","        return f\n","    \n","    def loss_func(self):\n","        u_pred = self.net_u(self.x, self.t)\n","        f_pred = self.net_f(self.x, self.t)\n","        loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n","        \n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        \n","        self.iter += 1\n","        if self.iter % 100 == 0:\n","            print(\n","                'Loss: %e, l1: %.5f, l2: %.5f' % \n","                (\n","                    loss.item(), \n","                    self.lambda_1.item(), \n","                    torch.exp(self.lambda_2.detach()).item()\n","                )\n","            )\n","        return loss\n","    \n","    def train(self, nIter):\n","        self.dnn.train()\n","        for epoch in range(nIter):\n","            u_pred = self.net_u(self.x, self.t)\n","            f_pred = self.net_f(self.x, self.t)\n","            loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n","            \n","            # Backward and optimize\n","            # 第1个优化器训练 次\n","            # ==============优化器1：adam =====================\n","            # self.optimizer_Adam.zero_grad()\n","            # loss.backward()\n","            # self.optimizer_Adam.step()\n","            \n","            # ==============优化器2：adahessian =====================\n","            self.optimizer_Adahessian.zero_grad()\n","            loss.backward(create_graph = True)\n","            self.optimizer_Adahessian.step()\n","            \n","            if epoch % 100 == 0:\n","                print(\n","                    'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n","                    (\n","                        epoch, \n","                        loss.item(), \n","                        self.lambda_1.item(), \n","                        torch.exp(self.lambda_2).item()\n","                    )\n","                )\n","                \n","        # Backward and optimize\n","        # 第二个优化器训练50000次\n","        self.optimizer.step(self.loss_func)\n","    \n","    def predict(self, X):\n","        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n","        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n","\n","        self.dnn.eval()\n","        u = self.net_u(x, t)\n","        f = self.net_f(x, t)\n","        u = u.detach().cpu().numpy()\n","        f = f.detach().cpu().numpy()\n","        return u, f"]},{"cell_type":"markdown","metadata":{"id":"mzXbfVvApLkK"},"source":["## Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3blUH8gpLkK"},"outputs":[],"source":["nu = 0.01/np.pi\n","\n","N_u = 2000\n","layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n","\n","data = scipy.io.loadmat('data/burgers_shock.mat')\n","\n","t = data['t'].flatten()[:,None]\n","x = data['x'].flatten()[:,None]\n","Exact = np.real(data['usol']).T\n","\n","X, T = np.meshgrid(x,t)\n","\n","X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n","u_star = Exact.flatten()[:,None]              \n","\n","# Doman bounds\n","lb = X_star.min(0)\n","ub = X_star.max(0) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"WbpDSsQUpLkL"},"source":["## Training on Non-noisy Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":559},"id":"dT1GDbKxpLkM","executionInfo":{"status":"error","timestamp":1653307899026,"user_tz":-480,"elapsed":27490,"user":{"displayName":"xiong xiong","userId":"17756750327085610403"}},"outputId":"f493b761-5f26-4a22-ce49-c0d798cbcb8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["It: 0, Loss: 4.108e-01, Lambda_1: 0.001, Lambda_2: 0.002479\n","It: 100, Loss: 3.846e-01, Lambda_1: -0.189, Lambda_2: 0.002360\n","It: 200, Loss: 3.846e-01, Lambda_1: -0.189, Lambda_2: 0.002360\n","It: 300, Loss: 3.846e-01, Lambda_1: -0.189, Lambda_2: 0.002360\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-5544cf3242cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nnoise = 0.0            \\n\\n# create training set\\nidx = np.random.choice(X_star.shape[0], N_u, replace=False)\\nX_u_train = X_star[idx,:]\\nu_train = u_star[idx,:]\\n\\n# training\\nmodel = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\\nmodel.train(10000)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-9-e25bd41e3ba3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nIter)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# ==============优化器2：adahessian =====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_Adahessian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_Adahessian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.47 GiB already allocated; 3.75 MiB free; 13.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["%%time\n","\n","noise = 0.0            \n","\n","# create training set\n","idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n","X_u_train = X_star[idx,:]\n","u_train = u_star[idx,:]\n","\n","# training\n","model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n","model.train(10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYFG6JlhpLkN"},"outputs":[],"source":["# evaluations\n","u_pred, f_pred = model.predict(X_star)\n","\n","error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n","\n","U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n","\n","lambda_1_value = model.lambda_1.detach().cpu().numpy()\n","lambda_2_value = model.lambda_2.detach().cpu().numpy()\n","lambda_2_value = np.exp(lambda_2_value)\n","\n","error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n","error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n","\n","print('Error u: %e' % (error_u))    \n","print('Error l1: %.5f%%' % (error_lambda_1))                             \n","print('Error l2: %.5f%%' % (error_lambda_2))  "]},{"cell_type":"markdown","metadata":{"id":"7SmOfaGwpLkO"},"source":["## Training on Noisy Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WL_GXGCvpLkP"},"outputs":[],"source":["noise = 0.01    \n","\n","# create training set\n","u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n","\n","# training\n","model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n","model.train(10000)  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"9g3lzjbBpLkP"},"source":["## Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2bUBtVqpLkQ"},"outputs":[],"source":["\n","\"\"\" The aesthetic setting has changed. \"\"\"\n","\n","####### Row 0: u(t,x) ##################    \n","\n","fig = plt.figure(figsize=(9, 5))\n","ax = fig.add_subplot(111)\n","\n","h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n","              extent=[t.min(), t.max(), x.min(), x.max()], \n","              origin='lower', aspect='auto')\n","divider = make_axes_locatable(ax)\n","cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n","cbar = fig.colorbar(h, cax=cax)\n","cbar.ax.tick_params(labelsize=15) \n","\n","ax.plot(\n","    X_u_train[:,1], \n","    X_u_train[:,0], \n","    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n","    markersize = 4,  # marker size doubled\n","    clip_on = False,\n","    alpha=.5\n",")\n","\n","line = np.linspace(x.min(), x.max(), 2)[:,None]\n","ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","\n","ax.set_xlabel('$t$', size=20)\n","ax.set_ylabel('$x$', size=20)\n","ax.legend(\n","    loc='upper center', \n","    bbox_to_anchor=(0.9, -0.05), \n","    ncol=5, \n","    frameon=False, \n","    prop={'size': 15}\n",")\n","ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n","ax.tick_params(labelsize=15)\n","\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42rO_0kwpLkQ"},"outputs":[],"source":["####### Row 1: u(t,x) slices ################## \n","\n","\"\"\" The aesthetic setting has changed. \"\"\"\n","\n","fig = plt.figure(figsize=(14, 10))\n","ax = fig.add_subplot(111)\n","\n","gs1 = gridspec.GridSpec(1, 3)\n","gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n","\n","ax = plt.subplot(gs1[0, 0])\n","ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')    \n","ax.set_title('$t = 0.25$', fontsize = 15)\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","ax = plt.subplot(gs1[0, 1])\n","ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])\n","ax.set_title('$t = 0.50$', fontsize = 15)\n","ax.legend(\n","    loc='upper center', \n","    bbox_to_anchor=(0.5, -0.15), \n","    ncol=5, \n","    frameon=False, \n","    prop={'size': 15}\n",")\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","ax = plt.subplot(gs1[0, 2])\n","ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])    \n","ax.set_title('$t = 0.75$', fontsize = 15)\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-yRnP8NpLkR"},"outputs":[],"source":["# evaluations\n","u_pred, f_pred = model.predict(X_star)\n","\n","lambda_1_value_noisy = model.lambda_1.detach().cpu().numpy()\n","lambda_2_value_noisy = model.lambda_2.detach().cpu().numpy()\n","lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n","\n","error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0) * 100\n","error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu) / nu * 100\n","\n","print('Error u: %e' % (error_u))    \n","print('Error l1: %.5f%%' % (error_lambda_1_noisy))                             \n","print('Error l2: %.5f%%' % (error_lambda_2_noisy))    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X2UqVbMpLkS"},"outputs":[],"source":["####### Row 3: Identified PDE ##################    \n","\n","fig = plt.figure(figsize=(14, 10))\n","\n","gs2 = gridspec.GridSpec(1, 3)\n","gs2.update(top=0.25, bottom=0, left=0.0, right=1.0, wspace=0.0)\n","\n","ax = plt.subplot(gs2[:, :])\n","ax.axis('off')\n","\n","s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n","s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n","s3 = r'Identified PDE (1\\% noise) & '\n","s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n","s5 = r'\\end{tabular}$'\n","s = s1+s2+s3+s4+s5\n","ax.text(0.1, 0.1, s, size=25)\n","\n","plt.show()\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Burgers Identification (PyTorch).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}